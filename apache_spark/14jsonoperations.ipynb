{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1586a3bc-3a3c-484c-8af2-3521421e2c1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/11 23:23:34 WARN Utils: Your hostname, Aadityas-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.1.128 instead (on interface en0)\n",
      "25/01/11 23:23:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/11 23:23:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/01/11 23:23:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/01/11 23:23:35 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/01/11 23:23:35 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/01/11 23:23:35 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.128:4044\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Reading and Parsing JSON Files/Data</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x11300e950>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/11 23:23:50 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Reading and Parsing JSON Files/Data\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50d46f04-c70c-4315-bbd4-58253466a7e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_single = spark.read.format(\"json\").load(\"datasets/order_singleline.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ddb5504-b679-48da-9d4b-2c865128e320",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- contact: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_line_items: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- amount: double (nullable = true)\n",
      " |    |    |-- item_id: string (nullable = true)\n",
      " |    |    |-- qty: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_single.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80ca6b59-df18-4a92-852e-d39915eb3354",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# contact vitra array xa array ko element ko dtatype long xa\n",
    "# order line items ni array xa \n",
    "#array vitra structure or simply dictionary xa tesko key amount ma double data type xa\n",
    "# key item_id string xa\n",
    "# key qty chai long xa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8c309a2-39e4-4b9f-bc2b-798d4cf85d09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------+--------------------+\n",
      "|             contact|customer_id|order_id|    order_line_items|\n",
      "+--------------------+-----------+--------+--------------------+\n",
      "|[9000010000, 9000...|       C001|    O101|[{102.45, I001, 6...|\n",
      "+--------------------+-----------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_single.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "638db773-4dbd-4fd6-bb93-7d8e69541277",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-----------+--------+------------------------------------+\n",
      "|contact                 |customer_id|order_id|order_line_items                    |\n",
      "+------------------------+-----------+--------+------------------------------------+\n",
      "|[9000010000, 9000010001]|C001       |O101    |[{102.45, I001, 6}, {2.01, I003, 2}]|\n",
      "+------------------------+-----------+--------+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tyo .... hataunu paryo vanae show garda truncate=False gardini\n",
    "df_single.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a2cd412-8242-4a8a-8035-ef88ec610b29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#using schema to read only three columns\n",
    "_schema = \"customer_id string, order_id string, contact array<long>\"\n",
    "\n",
    "df_schema = spark.read.format(\"json\").schema(_schema).load(\"datasets/order_singleline.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bce8352-c738-4375-aa41-72d1b78b8214",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+--------------------+\n",
      "|customer_id|order_id|             contact|\n",
      "+-----------+--------+--------------------+\n",
      "|       C001|    O101|[9000010000, 9000...|\n",
      "+-----------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_schema.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f28c77c1-ed7e-4592-ba27-0b284269e655",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#complex schema\n",
    "_schema = \"contact array<string>, customer_id string, order_id string, order_line_items array<struct<amount double, item_id string, qty long>>\"\n",
    "#note yesko column name ra json ko key same chai rakhni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c728414-21f7-46dd-b2d1-2cb2da6d9251",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_schema_new = spark.read.format(\"json\").schema(_schema).load(\"datasets/order_singleline.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0098bb2b-2e70-49b0-b818-6cf633173cd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- contact: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_line_items: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- amount: double (nullable = true)\n",
      " |    |    |-- item_id: string (nullable = true)\n",
      " |    |    |-- qty: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_schema_new.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00757c2d-6afc-415c-982e-afba790e2cf2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# imp\n",
    "### **Notes on `explode` and `.*` in PySpark**\n",
    "\n",
    "---\n",
    "\n",
    "### **`explode`**\n",
    "`explode` is used to transform a column containing arrays or dictionaries into multiple rows. It is particularly useful when dealing with nested or complex data structures.\n",
    "\n",
    "#### **Key Points:**\n",
    "1. **Arrays:**\n",
    "   - Each element in the array becomes a new row.\n",
    "2. **Dictionaries:**\n",
    "   - Each key-value pair becomes a new row with the key and value split into separate columns.\n",
    "3. **Null or Empty Data:**\n",
    "   - Null or empty arrays and dictionaries are skipped during the explosion.\n",
    "\n",
    "#### **Syntax:**\n",
    "```python\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "# For arrays\n",
    "df_exploded = df.select(\"column_name\", explode(\"array_column\").alias(\"exploded_value\"))\n",
    "\n",
    "# For dictionaries\n",
    "df_exploded = df.select(\"column_name\", explode(\"map_column\").alias(\"key\", \"value\"))\n",
    "```\n",
    "\n",
    "#### **Examples:**\n",
    "\n",
    "1. **Array Example:**\n",
    "   ```python\n",
    "   data = [(\"John\", [1, 2, 3]), (\"Jane\", [4, 5]), (\"Doe\", [])]\n",
    "   df = spark.createDataFrame(data, [\"name\", \"numbers\"])\n",
    "   df_exploded = df.select(\"name\", explode(\"numbers\").alias(\"number\"))\n",
    "   ```\n",
    "   **Result:**\n",
    "   | name | number |\n",
    "   |------|--------|\n",
    "   | John | 1      |\n",
    "   | John | 2      |\n",
    "   | John | 3      |\n",
    "   | Jane | 4      |\n",
    "   | Jane | 5      |\n",
    "\n",
    "2. **Dictionary Example:**\n",
    "   ```python\n",
    "   data = [(\"John\", {\"a\": 1, \"b\": 2}), (\"Jane\", {\"x\": 10, \"y\": 20}), (\"Doe\", None)]\n",
    "   df = spark.createDataFrame(data, [\"name\", \"attributes\"])\n",
    "   df_exploded = df.select(\"name\", explode(\"attributes\").alias(\"key\", \"value\"))\n",
    "   ```\n",
    "   **Result:**\n",
    "   | name | key | value |\n",
    "   |------|-----|-------|\n",
    "   | John | a   | 1     |\n",
    "   | John | b   | 2     |\n",
    "   | Jane | x   | 10    |\n",
    "   | Jane | y   | 20    |\n",
    "\n",
    "---\n",
    "\n",
    "### **`.*`**\n",
    "`.*` is a shorthand used in PySpark to select all the fields within a struct column and bring them to the top level as individual columns.\n",
    "\n",
    "#### **Key Points:**\n",
    "1. **Used with Struct Columns:**\n",
    "   - Extracts all fields of a struct into separate columns.\n",
    "2. **Simplifies Column Selection:**\n",
    "   - No need to manually select each field of a struct column.\n",
    "3. **Works Only with Structs:**\n",
    "   - If applied to a non-struct column, it will raise an error.\n",
    "\n",
    "#### **Syntax:**\n",
    "```python\n",
    "df_selected = df.select(\"column_name\", \"struct_column.*\")\n",
    "```\n",
    "\n",
    "#### **Example Using `.*`:**\n",
    "\n",
    "Instead of manually specifying each field like this:\n",
    "```python\n",
    "df.select(\n",
    "    \"contact\",\n",
    "    \"customer_id\",\n",
    "    \"order_id\",\n",
    "    df.expanded_line_items.item_name,\n",
    "    df.expanded_line_items.quantity,\n",
    "    df.expanded_line_items.price\n",
    ")\n",
    "```\n",
    "\n",
    "You can write:\n",
    "```python\n",
    "df.select(\n",
    "    \"contact\",\n",
    "    \"customer_id\",\n",
    "    \"order_id\",\n",
    "    \"expanded_line_items.*\"\n",
    ")\n",
    "```\n",
    "\n",
    "**Result:**\n",
    "| contact | customer_id | order_id | item_name | quantity | price |\n",
    "|---------|-------------|----------|-----------|----------|-------|\n",
    "| ...     | ...         | ...      | ...       | ...      | ...   |\n",
    "\n",
    "---\n",
    "\n",
    "### **Comparison of `explode` and `.*`**\n",
    "| Feature                  | `explode`                         | `.*`                          |\n",
    "|--------------------------|------------------------------------|-------------------------------|\n",
    "| **Use Case**             | Flatten arrays or dictionaries    | Extract fields from structs   |\n",
    "| **Output**               | Multiple rows                    | Multiple columns              |\n",
    "| **Applies To**           | Arrays, dictionaries              | Structs                       |\n",
    "| **Behavior**             | Creates a row for each element    | Expands fields into columns   |\n",
    "\n",
    "Let me know if you need clarifications or further examples! 😊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b02e8665-60b4-48a8-ae0c-1c892e05da3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#so explode le pratyek element ko lagi yeuta yeuta row banauxa\n",
    "#.* le chai dictionary ko key le column ko name ra value le chai value banauxa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8399a8f-b73b-4f08-a2c1-13e553c26bb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_one = spark.read.format(\"json\").schema(_schema).load(\"datasets/order_singleline.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e56d4510-fbea-4374-b860-02e9f81d6e2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------+--------------------+\n",
      "|             contact|customer_id|order_id|    order_line_items|\n",
      "+--------------------+-----------+--------+--------------------+\n",
      "|[9000010000, 9000...|       C001|    O101|[{102.45, I001, 6...|\n",
      "+--------------------+-----------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_one.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43a705a1-4eba-4f5d-bd4c-a5460c6daaa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#contact lai 2 ta row banai dim\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "df_two=df_one.select(explode(df_one[\"contact\"]).alias(\"number\"),df_one[\"customer_id\"],df_one[\"order_id\"],df_one[\"order_line_items\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e316c95b-e0c9-437a-9dba-1df6e67d36df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------+--------------------+\n",
      "|    number|customer_id|order_id|    order_line_items|\n",
      "+----------+-----------+--------+--------------------+\n",
      "|9000010000|       C001|    O101|[{102.45, I001, 6...|\n",
      "|9000010001|       C001|    O101|[{102.45, I001, 6...|\n",
      "+----------+-----------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_two.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd328a2c-ca5f-480f-95c0-ed7aaaf7b1fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#order_line_items lai ni ta explode garnu paryo ni\n",
    "df_three=df_two.select(df_two[\"customer_id\"],df_two[\"order_id\"],df_two[\"number\"],explode(df_two[\"order_line_items\"]).alias(\"order_line_items\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc6d5e61-e6df-425d-8858-9084d9cb1172",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------+-----------------+\n",
      "|customer_id|order_id|    number| order_line_items|\n",
      "+-----------+--------+----------+-----------------+\n",
      "|       C001|    O101|9000010000|{102.45, I001, 6}|\n",
      "|       C001|    O101|9000010000|  {2.01, I003, 2}|\n",
      "|       C001|    O101|9000010001|{102.45, I001, 6}|\n",
      "|       C001|    O101|9000010001|  {2.01, I003, 2}|\n",
      "+-----------+--------+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_three.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e8a09ef2-d8fd-42c7-8f89-01d22e9df6ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# so aaru value same rakhae ra number chai 2 ta vako xa\n",
    "#aaba tyo dictionary lai chai expand gardim\n",
    "tf_four=df_three.select(df_three[\"number\"],df_three[\"customer_id\"],df_three[\"order_id\"],df_three[\"order_line_items.qty\"])#aaba ta yeuta dictionary matrai xa so dictionaryname.key garni ,python ma chai name[\"key\"] garinxa tara yesma garda chai name.key garinxa\n",
    "#kun kun chaiya ho data json file ma herdai key indexing garni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a0571181-5a9d-4449-8fcb-438d9593aa92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------+---+\n",
      "|    number|customer_id|order_id|qty|\n",
      "+----------+-----------+--------+---+\n",
      "|9000010000|       C001|    O101|  6|\n",
      "|9000010000|       C001|    O101|  2|\n",
      "|9000010001|       C001|    O101|  6|\n",
      "|9000010001|       C001|    O101|  2|\n",
      "+----------+-----------+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_four.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "df91ca5e-cae3-482e-8d20-3b1b8655ff32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#aaba sabbai garnu pare \n",
    "tf_five=df_three.select(df_three[\"number\"],df_three[\"customer_id\"],df_three[\"order_id\"],\"order_line_items.*\")#.* garna lai chai dataframe[s.*] yesto na garni matrai s.* garni\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f5f0456e-878e-4107-b4e7-2c4528b834c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------+------+-------+---+\n",
      "|    number|customer_id|order_id|amount|item_id|qty|\n",
      "+----------+-----------+--------+------+-------+---+\n",
      "|9000010000|       C001|    O101|102.45|   I001|  6|\n",
      "|9000010000|       C001|    O101|  2.01|   I003|  2|\n",
      "|9000010001|       C001|    O101|102.45|   I001|  6|\n",
      "|9000010001|       C001|    O101|  2.01|   I003|  2|\n",
      "+----------+-----------+--------+------+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_five.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72959651-bff7-4a56-ba07-8ecfa59a4073",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparkvenv",
   "language": "python",
   "name": "sparkvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
