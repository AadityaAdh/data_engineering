{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c83b8cc-6a53-4b11-9c63-3c3050418fb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a921057d-48d6-420e-b6b3-7152c1a93112",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/11 16:39:54 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark=(\n",
    "SparkSession\n",
    "    .builder\n",
    "    .appName(\"creating datframe\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93f57bc2-5286-4891-b073-3cfedd2bfc99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Alice| 34|\n",
      "|  Bob| 36|\n",
      "|Cathy| 29|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#way to create a dataframe with data and column name\n",
    "data = [(\"Alice\", 34), (\"Bob\", 36), (\"Cathy\", 29)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b674d973-c8ed-4a84-ac93-1d446997f299",
   "metadata": {},
   "source": [
    "# reading from csv \n",
    "\n",
    "df = spark.read.csv(\"path/to/file.csv\", header=True, inferSchema=True)\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "\n",
    "\n",
    "In PySpark, the options `header=True` and `inferSchema=True` are used when reading data from files, such as CSV files, to control how the data is interpreted and loaded into a DataFrame.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. `header=True`**\n",
    "This option tells PySpark to treat the first row of the file as column headers.\n",
    "\n",
    "- **Purpose**: It ensures that the column names in the DataFrame match the header row of the CSV file instead of defaulting to generic column names like `_c0`, `_c1`, etc.\n",
    "- **Example**: If the CSV file looks like this:\n",
    "\n",
    "  ```\n",
    "  Name,Age,City\n",
    "  Alice,34,New York\n",
    "  Bob,36,Chicago\n",
    "  ```\n",
    "\n",
    "  With `header=True`, the DataFrame will look like:\n",
    "  ```\n",
    "  +-----+---+---------+\n",
    "  | Name|Age|     City|\n",
    "  +-----+---+---------+\n",
    "  |Alice| 34| New York|\n",
    "  |  Bob| 36|  Chicago|\n",
    "  +-----+---+---------+\n",
    "  ```\n",
    "\n",
    "  Without `header=True`, the first row of the CSV file is treated as data, and the column names default to `_c0`, `_c1`, etc.:\n",
    "  ```\n",
    "  +-----+---+---------+\n",
    "  |_c0  |_c1|     _c2 |\n",
    "  +-----+---+---------+\n",
    "  | Name|Age|     City|\n",
    "  |Alice| 34| New York|\n",
    "  |  Bob| 36|  Chicago|\n",
    "  +-----+---+---------+\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. `inferSchema=True`**\n",
    "This option tells PySpark to automatically detect the data types of columns based on their values.\n",
    "\n",
    "- **Purpose**: By default, PySpark treats all columns as `string` unless you explicitly specify the schema. `inferSchema=True` lets PySpark inspect the data and determine the appropriate types (e.g., integer, float, string, etc.).\n",
    "\n",
    "- **Example**: If the CSV file contains:\n",
    "  ```\n",
    "  Name,Age,City\n",
    "  Alice,34,New York\n",
    "  Bob,36,Chicago\n",
    "  ```\n",
    "\n",
    "  With `inferSchema=True`, the DataFrame will have these types:\n",
    "  ```\n",
    "  +-----+---+---------+\n",
    "  | Name|Age|     City|\n",
    "  +-----+---+---------+\n",
    "  |Alice| 34| New York|\n",
    "  |  Bob| 36|  Chicago|\n",
    "  +-----+---+---------+\n",
    "\n",
    "  Schema:\n",
    "  root\n",
    "   |-- Name: string (nullable = true)\n",
    "   |-- Age: integer (nullable = true)\n",
    "   |-- City: string (nullable = true)\n",
    "  ```\n",
    "\n",
    "  Without `inferSchema=True`, all columns will be treated as `string`:\n",
    "  ```\n",
    "  +-----+----+---------+\n",
    "  | Name| Age|     City|\n",
    "  +-----+----+---------+\n",
    "  |Alice|  34| New York|\n",
    "  |  Bob|  36|  Chicago|\n",
    "  +-----+----+---------+\n",
    "\n",
    "  Schema:\n",
    "  root\n",
    "   |-- Name: string (nullable = true)\n",
    "   |-- Age: string (nullable = true)\n",
    "   |-- City: string (nullable = true)\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### **When to Use Them**\n",
    "- **`header=True`**: Use when your file has meaningful column headers.\n",
    "- **`inferSchema=True`**: Use when you want PySpark to assign appropriate data types based on the file's content.\n",
    "\n",
    "---\n",
    "\n",
    "### **Usage Example**\n",
    "```python\n",
    "df = spark.read.csv(\"path/to/file.csv\", header=True, inferSchema=True)\n",
    "df.printSchema()\n",
    "df.show()\n",
    "```\n",
    "\n",
    "youtube ma krish nayak ko free code camp ma video xa tyo herni na bhujae yo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cb1394-bf94-48ef-81f4-94def0c62a53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparkvenv",
   "language": "python",
   "name": "sparkvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
