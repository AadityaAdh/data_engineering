{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c483895-ea80-4857-bc50-9a731abc4eec",
   "metadata": {
    "tags": []
   },
   "source": [
    "In PySpark, the `union()` function is used to combine two DataFrames that have the same schema (i.e., the same column names and types). It returns a new DataFrame containing the rows from both DataFrames.\n",
    "\n",
    "### **Key Points about `union()`**:\n",
    "- **Same Schema**: The DataFrames involved in a `union()` operation must have the same number of columns and matching column names and data types. If they don't match, you will get an error.\n",
    "- **Duplicates**: By default, `union()` includes duplicate rows from both DataFrames. If you want to remove duplicates, you can use `distinct()` after the `union()`.\n",
    "- **Order**: The rows are not guaranteed to be in any specific order after the union.\n",
    "\n",
    "---\n",
    "\n",
    "### **Syntax**:\n",
    "```python\n",
    "DataFrame.union(other)\n",
    "```\n",
    "\n",
    "- `other`: The DataFrame that will be combined with the original DataFrame.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example of Using `union()`**:\n",
    "\n",
    "#### **1. Union of Two DataFrames with Same Schema**\n",
    "\n",
    "```python\n",
    "# Sample DataFrames\n",
    "data1 = [(\"Alice\", 30), (\"Bob\", 25)]\n",
    "data2 = [(\"Charlie\", 35), (\"David\", 28)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "\n",
    "df1 = spark.createDataFrame(data1, columns)\n",
    "df2 = spark.createDataFrame(data2, columns)\n",
    "\n",
    "# Perform the union\n",
    "df_union = df1.union(df2)\n",
    "\n",
    "# Show the result\n",
    "df_union.show()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "+-------+---+\n",
    "|   Name|Age|\n",
    "+-------+---+\n",
    "|  Alice| 30|\n",
    "|    Bob| 25|\n",
    "|Charlie| 35|\n",
    "|  David| 28|\n",
    "+-------+---+\n",
    "```\n",
    "\n",
    "#### **2. Removing Duplicates after Union**\n",
    "\n",
    "To remove duplicates, you can chain `.distinct()` after the union:\n",
    "\n",
    "```python\n",
    "# Union and remove duplicates\n",
    "df_union_distinct = df1.union(df2).distinct()\n",
    "\n",
    "df_union_distinct.show()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "+-------+---+\n",
    "|   Name|Age|\n",
    "+-------+---+\n",
    "|  Alice| 30|\n",
    "|    Bob| 25|\n",
    "|Charlie| 35|\n",
    "|  David| 28|\n",
    "+-------+---+\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **What Happens if the Schemas Don't Match?**\n",
    "If the DataFrames have different column names or data types, you'll encounter an error. In such cases, you need to either:\n",
    "1. Ensure the schemas match.\n",
    "2. Rename columns or cast data types to make the schemas compatible.\n",
    "\n",
    "---\n",
    "\n",
    "### **Common Use Case**\n",
    "`union()` is commonly used in scenarios like combining data from multiple sources (e.g., multiple CSV files or database queries) into a single DataFrame for further processing.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "UNION:\n",
    "Definition: The UNION operation is used to combine the rows of two DataFrames with the same schema (same columns and data types). It doesn't check for matching values or conditions.\n",
    "Result: The result of a UNION operation will have rows from both DataFrames. If the DataFrames have the same rows, UNION will include all rows, including duplicates unless distinct() is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "894dd529-37e3-4a5b-abde-39816e43572c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#union vanae ko duita data lai yekkai thau ma jodni vanya jasto like append\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Sort Union & Aggregation\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ab10509-cf78-45f7-b012-b4f57533583f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emp_data_1 = [\n",
    "    [\"001\",\"101\",\"John Doe\",\"30\",\"Male\",\"50000\",\"2015-01-01\"],\n",
    "    [\"002\",\"101\",\"Jane Smith\",\"25\",\"Female\",\"45000\",\"2016-02-15\"],\n",
    "    [\"003\",\"102\",\"Bob Brown\",\"35\",\"Male\",\"55000\",\"2014-05-01\"],\n",
    "    [\"004\",\"102\",\"Alice Lee\",\"28\",\"Female\",\"48000\",\"2017-09-30\"],\n",
    "    [\"005\",\"103\",\"Jack Chan\",\"40\",\"Male\",\"60000\",\"2013-04-01\"],\n",
    "    [\"006\",\"103\",\"Jill Wong\",\"32\",\"Female\",\"52000\",\"2018-07-01\"],\n",
    "    [\"007\",\"101\",\"James Johnson\",\"42\",\"Male\",\"70000\",\"2012-03-15\"],\n",
    "    [\"008\",\"102\",\"Kate Kim\",\"29\",\"Female\",\"51000\",\"2019-10-01\"],\n",
    "    [\"009\",\"103\",\"Tom Tan\",\"33\",\"Male\",\"58000\",\"2016-06-01\"],\n",
    "    [\"010\",\"104\",\"Lisa Lee\",\"27\",\"Female\",\"47000\",\"2018-08-01\"]\n",
    "]\n",
    "\n",
    "emp_data_2 = [\n",
    "    [\"011\",\"104\",\"David Park\",\"38\",\"Male\",\"65000\",\"2015-11-01\"],\n",
    "    [\"012\",\"105\",\"Susan Chen\",\"31\",\"Female\",\"54000\",\"2017-02-15\"],\n",
    "    [\"013\",\"106\",\"Brian Kim\",\"45\",\"Male\",\"75000\",\"2011-07-01\"],\n",
    "    [\"014\",\"107\",\"Emily Lee\",\"26\",\"Female\",\"46000\",\"2019-01-01\"],\n",
    "    [\"015\",\"106\",\"Michael Lee\",\"37\",\"Male\",\"63000\",\"2014-09-30\"],\n",
    "    [\"016\",\"107\",\"Kelly Zhang\",\"30\",\"Female\",\"49000\",\"2018-04-01\"],\n",
    "    [\"017\",\"105\",\"George Wang\",\"34\",\"Male\",\"57000\",\"2016-03-15\"],\n",
    "    [\"018\",\"104\",\"Nancy Liu\",\"29\",\"\",\"50000\",\"2017-06-01\"],\n",
    "    [\"019\",\"103\",\"Steven Chen\",\"36\",\"Male\",\"62000\",\"2015-08-01\"],\n",
    "    [\"020\",\"102\",\"Grace Kim\",\"32\",\"Female\",\"53000\",\"2018-11-01\"]\n",
    "]\n",
    "\n",
    "emp_schema = \"employee_id string, department_id string, name string, age string, gender string, salary string, hire_date string\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d9ed260-afe4-4925-bcf5-89dbb4d39dcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emp_data_1 = spark.createDataFrame(data=emp_data_1, schema=emp_schema)\n",
    "emp_data_2 = spark.createDataFrame(data=emp_data_2, schema=emp_schema)\n",
    "#note same schema use garya xa\n",
    "#column ko order pani yeutai hunu parxa\n",
    "#yedi column ko order milae na vanae .select gari milauni aani union garni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3aa666c-307f-42e2-b691-340191c65573",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "|employee_id|department_id|         name|age|gender|salary| hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n",
      "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n",
      "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n",
      "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n",
      "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n",
      "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n",
      "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|\n",
      "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n",
      "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n",
      "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n",
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "\n",
      "+-----------+-------------+-----------+---+------+------+----------+\n",
      "|employee_id|department_id|       name|age|gender|salary| hire_date|\n",
      "+-----------+-------------+-----------+---+------+------+----------+\n",
      "|        011|          104| David Park| 38|  Male| 65000|2015-11-01|\n",
      "|        012|          105| Susan Chen| 31|Female| 54000|2017-02-15|\n",
      "|        013|          106|  Brian Kim| 45|  Male| 75000|2011-07-01|\n",
      "|        014|          107|  Emily Lee| 26|Female| 46000|2019-01-01|\n",
      "|        015|          106|Michael Lee| 37|  Male| 63000|2014-09-30|\n",
      "|        016|          107|Kelly Zhang| 30|Female| 49000|2018-04-01|\n",
      "|        017|          105|George Wang| 34|  Male| 57000|2016-03-15|\n",
      "|        018|          104|  Nancy Liu| 29|      | 50000|2017-06-01|\n",
      "|        019|          103|Steven Chen| 36|  Male| 62000|2015-08-01|\n",
      "|        020|          102|  Grace Kim| 32|Female| 53000|2018-11-01|\n",
      "+-----------+-------------+-----------+---+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_data_1.show()\n",
    "emp_data_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec7a23b8-0429-4da5-8f04-5521538df0d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "|employee_id|department_id|         name|age|gender|salary| hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n",
      "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n",
      "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n",
      "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n",
      "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n",
      "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n",
      "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|\n",
      "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n",
      "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n",
      "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n",
      "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|\n",
      "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|\n",
      "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|\n",
      "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|\n",
      "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|\n",
      "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|\n",
      "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|\n",
      "|        018|          104|    Nancy Liu| 29|      | 50000|2017-06-01|\n",
      "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|\n",
      "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|\n",
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# UNION and UNION ALL\n",
    "# select * from emp_data_1 UNION select * from emp_data_2\n",
    "emp = emp_data_1.unionAll(emp_data_2)\n",
    "emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6dd501f-a119-4141-9cea-326ce5b6148f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sort the emp data based on desc Salary\n",
    "# select * from emp order by salary desc\n",
    "from pyspark.sql.functions import desc, asc,expr\n",
    "\n",
    "emp_sorted = emp.orderBy(emp[\"salary\"].asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b15775b-ee08-4488-9c24-7e52d19cfa37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Aggregation with having clause\n",
    "# select dept_id, avg(salary) as avg_dept_salary from emp_sorted  group by dept_id having avg(salary) > 50000\n",
    "from pyspark.sql.functions import avg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8c8a7eb-8d3b-4a3e-9a2c-8f4f0a61b12b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#department ko average nilakna paila ta group garnu paryo aani aggregrate function launu paryo\n",
    "\n",
    "emp_avg_1=emp_sorted.groupBy(\"department_id\").agg(expr(\"avg(salary) as average_department_salary\")).where(\"average_department_salary>50000\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c201948e-b5fa-4e12-b3fe-bbe5b2e3d422",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------------+\n",
      "|department_id|average_department_salary|\n",
      "+-------------+-------------------------+\n",
      "|          101|                  55000.0|\n",
      "|          104|                  54000.0|\n",
      "|          102|                  51750.0|\n",
      "|          103|                  58000.0|\n",
      "|          106|                  69000.0|\n",
      "|          105|                  55500.0|\n",
      "+-------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_avg_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2355a291-1c9e-4488-8ab9-646c10d35052",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparkvenv",
   "language": "python",
   "name": "sparkvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
